
@article{lacoste-julienConvergenceRateFrankWolfe2016,
	title = {Convergence Rate of Frank-Wolfe for Non-Convex Objectives},
	url = {http://arxiv.org/abs/1607.00345},
	abstract = {We give a simple proof that the Frank-Wolfe algorithm obtains a stationary point at a rate of \$O(1/{\textbackslash}sqrt\{t\})\$ on non-convex objectives with a Lipschitz continuous gradient. Our analysis is affine invariant and is the first, to the best of our knowledge, giving a similar rate to what was already proven for projected gradient methods (though on slightly different measures of stationarity).},
	journaltitle = {{arXiv}:1607.00345 [cs, math, stat]},
	author = {Lacoste-Julien, Simon},
	urldate = {2020-04-08},
	date = {2016-07-01},
	eprinttype = {arxiv},
	eprint = {1607.00345},
	file = {Lacoste-Julien_2016_Convergence Rate of Frank-Wolfe for Non-Convex Objectives.pdf:/Users/wpq/Dropbox (MIT)/zotero/Lacoste-Julien_2016_Convergence Rate of Frank-Wolfe for Non-Convex Objectives.pdf:application/pdf}
}

@article{jinNonconvexOptimizationMachine2019,
	title = {On Nonconvex Optimization for Machine Learning: Gradients, Stochasticity, and Saddle Points},
	url = {http://arxiv.org/abs/1902.04811},
	shorttitle = {On Nonconvex Optimization for Machine Learning},
	abstract = {Gradient descent ({GD}) and stochastic gradient descent ({SGD}) are the workhorses of large-scale machine learning. While classical theory focused on analyzing the performance of these methods in convex optimization problems, the most notable successes in machine learning have involved nonconvex optimization, and a gap has arisen between theory and practice. Indeed, traditional analyses of {GD} and {SGD} show that both algorithms converge to stationary points efficiently. But these analyses do not take into account the possibility of converging to saddle points. More recent theory has shown that {GD} and {SGD} can avoid saddle points, but the dependence on dimension in these analyses is polynomial. For modern machine learning, where the dimension can be in the millions, such dependence would be catastrophic. We analyze perturbed versions of {GD} and {SGD} and show that they are truly efficient---their dimension dependence is only polylogarithmic. Indeed, these algorithms converge to second-order stationary points in essentially the same time as they take to converge to classical first-order stationary points.},
	journaltitle = {{arXiv}:1902.04811 [cs, math, stat]},
	author = {Jin, Chi and Netrapalli, Praneeth and Ge, Rong and Kakade, Sham M. and Jordan, Michael I.},
	urldate = {2020-04-15},
	date = {2019-09-03},
	eprinttype = {arxiv},
	eprint = {1902.04811},
	file = {Jin et al_2019_On Nonconvex Optimization for Machine Learning - Gradients, Stochasticity, and Saddle Points.pdf:/Users/wpq/Dropbox (MIT)/zotero/Jin et al_2019_On Nonconvex Optimization for Machine Learning - Gradients, Stochasticity, and Saddle Points.pdf:application/pdf}
}

@article{gorskiBiconvexSetsOptimization2007,
	title = {Biconvex sets and optimization with biconvex functions: a survey and extensions},
	volume = {66},
	issn = {1432-5217},
	url = {https://doi.org/10.1007/s00186-007-0161-1},
	doi = {10.1007/s00186-007-0161-1},
	shorttitle = {Biconvex sets and optimization with biconvex functions},
	abstract = {The problem of optimizing a biconvex function over a given (bi)convex or compact set frequently occurs in theory as well as in industrial applications, for example, in the field of multifacility location or medical image registration. Thereby, a function \$\$f:X{\textbackslash}times Y{\textbackslash}to\{{\textbackslash}mathbb\{R\}\}\$\$is called biconvex, if f(x,y) is convex in y for fixed x∈X, and f(x,y) is convex in x for fixed y∈Y. This paper presents a survey of existing results concerning the theory of biconvex sets and biconvex functions and gives some extensions. In particular, we focus on biconvex minimization problems and survey methods and algorithms for the constrained as well as for the unconstrained case. Furthermore, we state new theoretical results for the maximum of a biconvex function over biconvex sets.},
	pages = {373--407},
	number = {3},
	journaltitle = {Mathematical Methods of Operations Research},
	shortjournal = {Math Meth Oper Res},
	author = {Gorski, Jochen and Pfeuffer, Frank and Klamroth, Kathrin},
	urldate = {2020-06-14},
	date = {2007-12-01},
	langid = {english},
	file = {Gorski et al_2007_Biconvex sets and optimization with biconvex functions - a survey and extensions.pdf:/Users/wpq/Dropbox (MIT)/zotero/Gorski et al_2007_Biconvex sets and optimization with biconvex functions - a survey and extensions.pdf:application/pdf}
}